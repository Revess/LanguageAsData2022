{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2: Querying Word Vectors\n",
    "\n",
    "In this lab we learn how to represent words using a pre-trained model. We will work with the English fasttext model [wiki-news-300d-1M.vec](https://fasttext.cc/docs/en/english-vectors.html). Download it and save it to the data folder. \n",
    "\n",
    "You can download models for other languages [here](https://fasttext.cc/docs/en/crawl-vectors.html) at the bottom of the page. \n",
    "\n",
    "We use the *gensim* module to query the model. The model is quite big, so the following loading step takes long: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "print(\"loading\")\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"../data/wiki-news-300d-1M.vec\")\n",
    "print(\"done loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the vector representation for a word from the model. Note that the model only contains one million words. Some words can thus not be found and you will get a *KeyError*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term =\"veganism\"\n",
    "term_vector = word_vectors.get_vector(term)\n",
    "\n",
    "print(term_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculating similarity\n",
    "Gensim provides different options to calculate the similarity between words. The standard measure for determining the similarity between words is to calculate the cosine similarity between their vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term1 =\"vegetables\"\n",
    "term2 = \"fruit\"\n",
    "\n",
    "word_vectors.similarity(term1,term2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Arithmetic operations\n",
    "\n",
    "Word vectors have become famous because they make it possible to perform arithmetic operations over word vectors. The most popular example is: \n",
    "woman + king - man = queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term =\"veganism\"\n",
    "subtract =\"vegetarian\"\n",
    "word_vectors.most_similar(positive=[term], negative=[subtract])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training your own model\n",
    "\n",
    "The word vectors have been trained on Wikipedia. You can also train word vectors on your own dataset. **In this example, we only apply tokenization and lowercasing. Discuss which pre-processing steps you want to use for your dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "\n",
    "# Read in TSV\n",
    "tsv_file = \"../data/veganism_overview_en.tsv\"\n",
    "content = pd.read_csv(tsv_file, sep=\"\\t\", header = 0, keep_default_na=False)\n",
    "articles = content[\"Text\"]\n",
    "stanza.download(\"en\")\n",
    "tokenizer = stanza.Pipeline('en', processors='tokenize')\n",
    "tokenized = []\n",
    "for article in articles:\n",
    "    for sent in tokenizer(article).sentences:\n",
    "        tokenized.append([tok.text.lower() for tok in sent.tokens])\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Train a Word2Vec model, the min_count parameter indicates the minimum frequency of each word in the corpus\n",
    "mymodel = Word2Vec(tokenized, min_count=2)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(mymodel)\n",
    "\n",
    "\n",
    "# Let's have a look at the 50 most frequent words\n",
    "for index in range(50):\n",
    "    print(mymodel.wv.index_to_key[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.wv.most_similar(\"vegetarian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that function words are very dominant in this model because the dataset is so small. **Discuss how this affects the analysis for your own dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing word vectors\n",
    "\n",
    "Word vectors with 100 dimensions are difficult to conceptualize for human brains. As a simplification, it is common to apply a dimensionality reduction technique on the word vector to obtain a two-dimensional representation. Obviously, this two-dimensional vector contains much less information than the 100-dimensional vector. However, it can yield nice visualizations to provide anecdotal evidence.  \n",
    "\n",
    "Popular algorithm for calculating the reduced dimensions are principal component analysis (PCA) and [T-SNE](https://lvdmaaten.github.io/tsne/). \n",
    "\n",
    "**Play around with different models, reduction techniques and term selection. Note that the dimensionality reduction for the big fasttext model takes very long and you might run into memory errors. It could help to run the code in a regular python file (.py) instead of in a jupyter notebook.** [This article](https://towardsdatascience.com/why-you-are-using-t-sne-wrong-502412aab0c0) provides some good advice on interpreting T-SNE output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Select model \n",
    "model=word_vectors\n",
    "\n",
    "# If you use a large model, you need to restrict the vocabulary to the most frequent terms\n",
    "# If you use the full vocab it might cause a crash of the notebook\n",
    "# Restricted\n",
    "vocab = [model.index_to_key[i] for i in range(8000)]\n",
    "\n",
    "# Apply dimensionality reduction with PCA or T-SNE\n",
    "high_dimensional = [model[w] for w in vocab]\n",
    "reduction_technique = TSNE(n_components=2)\n",
    "#reduction_technique = PCA(n_components=2)\n",
    "\n",
    "print(\"Calculate dimensionality reduction\")\n",
    "two_dimensional = reduction_technique.fit_transform(high_dimensional)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices in the vocabulary for selected terms\n",
    "terms =[\"fruit\", \"meat\", \"salt\",\"eat\", \"healthy\", \"good\", \"sugar\", \"sweet\"]\n",
    "term_indices = [model.key_to_index[t] for t in terms]\n",
    "\n",
    "print(term_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 10))\n",
    "\n",
    "# Plot the two-dimensional vectors for the selected terms\n",
    "x_values = [two_dimensional[index, 0] for index in term_indices]\n",
    "y_values = [two_dimensional[index, 1] for index in term_indices]\n",
    "\n",
    "ax.plot(x_values, y_values, 'o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's improve the plot a bit. **Play around with the plotting options.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import numpy as np \n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 10))\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(terms)))\n",
    "for x, y, c in zip(x_values, y_values, colors):\n",
    "    ax.plot(x, y, 'o', markersize=12, color=c)\n",
    "\n",
    "# Add title and description\n",
    "ax.set_title('My Terms')\n",
    "description=\"The word vectors for selected terms in the English fasttext model (wiki-news-300d-1M.vec) reduced to two dimensions using the t-sne algorithm. \"\n",
    "fig.text(.51, .05, description, ha=\"center\", fontsize=12)\n",
    "\n",
    "# Hide the ticks \n",
    "ax.set_yticks([]) \n",
    "ax.set_xticks([])\n",
    "\n",
    "# Annotate the terms in the plot\n",
    "for i, word in enumerate(terms):\n",
    "\tplt.annotate(word, xy=(x_values[i], y_values[i]), fontsize = 12)\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
