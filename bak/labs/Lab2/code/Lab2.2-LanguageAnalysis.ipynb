{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2: Language Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the HLT course, you learned how to perform natural language processing steps using the libraries *nltk* and *spacy* in [Lab 1](https://github.com/cltl/ma-hlt-labs/tree/master/lab1.toolkits). Now is a good time to refresh your memory.  \n",
    "Spacy and NLTK are only available for a few languages. In this lab, we will work with *stanza* which is available for more than 60 languages. Take a look at the [documentation](https://stanfordnlp.github.io/stanza/) for details. Stanza is optimized for accuracy and not for speed, so processing takes longer than with spacy. \n",
    "\n",
    "You are free to choose any of the libraries for your assignments. Just make sure to document your selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c36070f20543a6b93972c653077eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 14:00:05 INFO: Downloading default packages for language: fr (French) ...\n",
      "2022-11-17 14:00:07 INFO: File exists: /home/revess/stanza_resources/fr/default.zip\n",
      "2022-11-17 14:00:11 INFO: Finished downloading models and saved to /home/revess/stanza_resources.\n",
      "2022-11-17 14:00:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce653cab5434599a8c57980fb11dfa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 14:00:13 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2022-11-17 14:00:13 INFO: Use device: cpu\n",
      "2022-11-17 14:00:13 INFO: Loading: tokenize\n",
      "2022-11-17 14:00:13 INFO: Loading: mwt\n",
      "2022-11-17 14:00:13 INFO: Loading: pos\n",
      "2022-11-17 14:00:13 INFO: Loading: lemma\n",
      "2022-11-17 14:00:13 INFO: Loading: depparse\n",
      "2022-11-17 14:00:14 INFO: Loading: ner\n",
      "2022-11-17 14:00:15 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "\n",
    "# Read in the data \n",
    "language = \"fr\"\n",
    "article_file = \"../data/veganism_overview_\" + language +\".tsv\"\n",
    "content = pd.read_csv(article_file, sep=\"\\t\", header = 0, keep_default_na=False)\n",
    "\n",
    "# Prepare the nlp pipeline\n",
    "stanza.download(language)\n",
    "nlp = stanza.Pipeline(language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-20</td>\n",
       "      <td>07:00:00Z</td>\n",
       "      <td>L'Indépendant</td>\n",
       "      <td></td>\n",
       "      <td>Le Veganisme va-t-il trop loin avec par exempl...</td>\n",
       "      <td>https://www.lindependant.fr/2020/09/20/le-vega...</td>\n",
       "      <td>Les véganeries vont de plus en plus loin. Conn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>07:00:00Z</td>\n",
       "      <td>Alternative Santé</td>\n",
       "      <td>Maria Fornell rédigé le 16 septembre 2020 à 11h47</td>\n",
       "      <td>Végétarisme, végétalisme, véganisme : les risq...</td>\n",
       "      <td>https://www.alternativesante.fr/vegetarisme/ve...</td>\n",
       "      <td>Pour ajouter cet article à vos favoris, veuil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>04:10:00Z</td>\n",
       "      <td>Ouest-France</td>\n",
       "      <td></td>\n",
       "      <td>La Ménitré. Il publie un roman futuriste sur l...</td>\n",
       "      <td>https://www.ouest-france.fr/pays-de-la-loire/l...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>14:38:32Z</td>\n",
       "      <td>Kawa News</td>\n",
       "      <td></td>\n",
       "      <td>Concilier culture orientale et véganisme, miss...</td>\n",
       "      <td>https://kawa-news.com/concilier-culture-orient...</td>\n",
       "      <td>Du Maghreb au Moyen-Orient, la viande s’invite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-25</td>\n",
       "      <td>14:05:43Z</td>\n",
       "      <td>Sud Ouest</td>\n",
       "      <td></td>\n",
       "      <td>Charente-Maritime : rassemblement des vegan de...</td>\n",
       "      <td>https://www.sudouest.fr/2020/09/25/charente-ma...</td>\n",
       "      <td>Ce samedi à Surgères, l’association 269 Life ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Europe1</td>\n",
       "      <td></td>\n",
       "      <td>En France, qui sont vraiment les vegans ?</td>\n",
       "      <td>https://www.europe1.fr/societe/en-france-qui-s...</td>\n",
       "      <td>C'est un mode de vie qui fait de plus en plus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2019-11-21</td>\n",
       "      <td>08:00:00Z</td>\n",
       "      <td>Parents.fr</td>\n",
       "      <td></td>\n",
       "      <td>Véganisme : des parents arrêtés après le décès...</td>\n",
       "      <td>https://www.parents.fr/actualites/bebe/veganis...</td>\n",
       "      <td>A 18 mois, il pesait le poids d’un enfant de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Le Figaro</td>\n",
       "      <td></td>\n",
       "      <td>Comment rester vegan, sans perdre ses amis ?</td>\n",
       "      <td>https://madame.lefigaro.fr/cuisine/comment-viv...</td>\n",
       "      <td>Plus on a du tofu, plus on rit ! Humeur. - Emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2019-12-09</td>\n",
       "      <td>08:00:00Z</td>\n",
       "      <td>POSITIVR</td>\n",
       "      <td>Mathilde Sallé de Chou</td>\n",
       "      <td>Véganisme : Million Dollar Vegan défie Donald ...</td>\n",
       "      <td>https://positivr.fr/veganisme-million-dollar-v...</td>\n",
       "      <td>Devenir végan pendant un mois contre un chèque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2020-08-24</td>\n",
       "      <td>09:37:59Z</td>\n",
       "      <td>Konbini Food</td>\n",
       "      <td></td>\n",
       "      <td>Devenez végan pendant un mois et empochez 2 50...</td>\n",
       "      <td>https://food.konbini.com/news/devenez-vegan-pe...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Publication Date       Time          Publisher  \\\n",
       "0        2020-09-20  07:00:00Z      L'Indépendant   \n",
       "1        2020-09-16  07:00:00Z  Alternative Santé   \n",
       "2        2020-09-29  04:10:00Z       Ouest-France   \n",
       "3        2020-09-22  14:38:32Z          Kawa News   \n",
       "4        2020-09-25  14:05:43Z          Sud Ouest   \n",
       "..              ...        ...                ...   \n",
       "95                                        Europe1   \n",
       "96       2019-11-21  08:00:00Z         Parents.fr   \n",
       "97                                      Le Figaro   \n",
       "98       2019-12-09  08:00:00Z           POSITIVR   \n",
       "99       2020-08-24  09:37:59Z       Konbini Food   \n",
       "\n",
       "                                               Author  \\\n",
       "0                                                       \n",
       "1   Maria Fornell rédigé le 16 septembre 2020 à 11h47   \n",
       "2                                                       \n",
       "3                                                       \n",
       "4                                                       \n",
       "..                                                ...   \n",
       "95                                                      \n",
       "96                                                      \n",
       "97                                                      \n",
       "98                             Mathilde Sallé de Chou   \n",
       "99                                                      \n",
       "\n",
       "                                                Title  \\\n",
       "0   Le Veganisme va-t-il trop loin avec par exempl...   \n",
       "1   Végétarisme, végétalisme, véganisme : les risq...   \n",
       "2   La Ménitré. Il publie un roman futuriste sur l...   \n",
       "3   Concilier culture orientale et véganisme, miss...   \n",
       "4   Charente-Maritime : rassemblement des vegan de...   \n",
       "..                                                ...   \n",
       "95          En France, qui sont vraiment les vegans ?   \n",
       "96  Véganisme : des parents arrêtés après le décès...   \n",
       "97       Comment rester vegan, sans perdre ses amis ?   \n",
       "98  Véganisme : Million Dollar Vegan défie Donald ...   \n",
       "99  Devenez végan pendant un mois et empochez 2 50...   \n",
       "\n",
       "                                                  URL  \\\n",
       "0   https://www.lindependant.fr/2020/09/20/le-vega...   \n",
       "1   https://www.alternativesante.fr/vegetarisme/ve...   \n",
       "2   https://www.ouest-france.fr/pays-de-la-loire/l...   \n",
       "3   https://kawa-news.com/concilier-culture-orient...   \n",
       "4   https://www.sudouest.fr/2020/09/25/charente-ma...   \n",
       "..                                                ...   \n",
       "95  https://www.europe1.fr/societe/en-france-qui-s...   \n",
       "96  https://www.parents.fr/actualites/bebe/veganis...   \n",
       "97  https://madame.lefigaro.fr/cuisine/comment-viv...   \n",
       "98  https://positivr.fr/veganisme-million-dollar-v...   \n",
       "99  https://food.konbini.com/news/devenez-vegan-pe...   \n",
       "\n",
       "                                                 Text  \n",
       "0   Les véganeries vont de plus en plus loin. Conn...  \n",
       "1    Pour ajouter cet article à vos favoris, veuil...  \n",
       "2                                                      \n",
       "3   Du Maghreb au Moyen-Orient, la viande s’invite...  \n",
       "4    Ce samedi à Surgères, l’association 269 Life ...  \n",
       "..                                                ...  \n",
       "95  C'est un mode de vie qui fait de plus en plus ...  \n",
       "96   A 18 mois, il pesait le poids d’un enfant de ...  \n",
       "97  Plus on a du tofu, plus on rit ! Humeur. - Emb...  \n",
       "98  Devenir végan pendant un mois contre un chèque...  \n",
       "99                                                     \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded the pipeline and can start processing our content. In the example, we only use the first article. **Once you understand how it works, modify the code to iterate through all articles and save the result.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first article\n",
    "current_article = content[\"Text\"][0]\n",
    "processed_article = nlp(current_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stanza pipeline detects sentence boundaries and segments the texts into tokens. **Analyze the output and check the tokenization quality.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Les véganeries vont de plus en plus loin.\n",
      "Tokens: \n",
      "1 Les\n",
      "2 véganeries\n",
      "3 vont\n",
      "4 de\n",
      "5 plus\n",
      "6 en\n",
      "7 plus\n",
      "8 loin\n",
      "9 .\n",
      "\n",
      "Sentence:  Connaissez-vous le \"faux mage\" à base de lait de noix de cajou ?\n",
      "Tokens: \n",
      "1 Connaissez\n",
      "2 -vous\n",
      "3 le\n",
      "4 \"\n",
      "5 faux\n",
      "6 mage\n",
      "7 \"\n",
      "8 à\n",
      "9 base\n",
      "10 de\n",
      "11 lait\n",
      "12 de\n",
      "13 noix\n",
      "14 de\n",
      "15 cajou\n",
      "16 ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = processed_article.sentences\n",
    "\n",
    "# For the example, I only look at the first three sentences. Make sure to change this. \n",
    "for sentence in sentences[0:2]:\n",
    "    print(\"Sentence: \", sentence.text)\n",
    "    print(\"Tokens: \")\n",
    "    for token in sentence.tokens:\n",
    "        print(token.id[0], token.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Token Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the token frequencies in this article. If necessary, go back to [chapter 10](https://github.com/cltl/python-for-text-analysis/blob/master/Chapters/Chapter%2010%20-%20Dictionaries.ipynb) of the python course and refresh your skills on how to count with dictionaries and counters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 34), (',', 31), ('la', 18), ('.', 17), ('des', 15), ('...', 14), ('et', 13), ('en', 12), ('?', 12), ('à', 11), ('pas', 10), ('le', 9), (\"l'\", 9), ('produits', 9), ('pour', 9), ('les', 9), (\"d'\", 7), ('est', 7), ('a', 6), ('vous', 6), ('qui', 6), ('ne', 6), ('ils', 6), ('plus', 5), ('lait', 5), ('cajou', 5), ('alors', 5), ('ça', 5), ('ou', 5), ('notre', 5), ('viande', 5), ('nous', 5), ('mais', 5), ('\"', 4), ('noix', 4), ('y', 4), ('soja', 4), ('un', 4), ('une', 4), (\"n'\", 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_frequencies = Counter()\n",
    "\n",
    "for sentence in sentences:\n",
    "    all_tokens =[token.text for token in sentence.tokens]\n",
    "    token_frequencies.update(all_tokens)\n",
    "    \n",
    "# Print the most common tokens\n",
    "print(token_frequencies.most_common(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type-token ratio is an indicator for lexical variation. **Think about example texts with very high or very low type-token ratio.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333 737\n",
      "0.4518\n"
     ]
    }
   ],
   "source": [
    "# Type-token ratio\n",
    "num_types = len(token_frequencies.keys())\n",
    "num_tokens = sum(token_frequencies.values())\n",
    "\n",
    "tt_ratio = num_types/num_tokens\n",
    "\n",
    "print(num_types, num_tokens)\n",
    "\n",
    "# Print the type token ratio with 4 decimals\n",
    "print(\"%.4f\" % tt_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save complex Python objects like dictionaries (or like the processed articles) in a *pickle* file. This can be convenient if you are running a processing step that takes a lot of time and you want to do it only once. You can then save the intermediate output in a pickle-file and load it directly when you continue working on it. \n",
    "\n",
    "Note that pickle files can also be used to hide harmful code. So make sure to only open pickle files if you know who created them and what they contain. More information can be found in this [tutorial](https://www.datacamp.com/community/tutorials/pickle-python-tutorial).\n",
    "\n",
    "When opening files, *w* stands for write, *r* for read and *b* indicates that the file is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "frequency_file = \"../data/processed_data/tokenfrequencies_article1.pkl\"\n",
    "pickle.dump(token_frequencies, open(frequency_file, \"wb\"))\n",
    "\n",
    "stanza_objects_file = \"../data/processed_data/nlp_article1.pkl\"\n",
    "pickle.dump(processed_article, open(stanza_objects_file, \"wb\"))\n",
    "\n",
    "# You can then later load the file like this: \n",
    "# loaded_frequencies = pickle.load(open(frequencyfile, \"rb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Token frequencies of all articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, we only used the first article. **Once you understand how it works, modify the code to iterate through all articles and save the result in \"../data/processed_data/tokenfrequencies.pkl\".** \n",
    "\n",
    "Stanza processing takes quite long, so you can try it directly with your own dataset, if you prefer. Do not save all stanza objects, the file will get quite big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "\n",
    "\n",
    "# Iterate through all articles\n",
    "    \n",
    "    # Skip empty articles\n",
    "    \n",
    "    # Process the article with the stanza pipeline\n",
    "    \n",
    "    # Iterate through all sentences of the article\n",
    "    \n",
    "        # Add the tokens to a counter\n",
    "\n",
    "# Save the token frequencies as a pickle file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. More linguistic processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline provides a large amount of additional information. Let's check the representation of the sentence. **Try to understand which information is represented by the attributes *lemma*, *upos*, *feats*, *heads*, *deprel* and *ner*.** We will learn more about this in the next week, but you can already collect some statistics over the information.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"text\": \"Les\",\n",
      "    \"lemma\": \"le\",\n",
      "    \"upos\": \"DET\",\n",
      "    \"feats\": \"Definite=Def|Number=Plur|PronType=Art\",\n",
      "    \"head\": 2,\n",
      "    \"deprel\": \"det\",\n",
      "    \"start_char\": 0,\n",
      "    \"end_char\": 3,\n",
      "    \"ner\": \"S-LOC\",\n",
      "    \"multi_ner\": [\n",
      "      \"S-LOC\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"text\": \"véganeries\",\n",
      "    \"lemma\": \"véganerie\",\n",
      "    \"upos\": \"NOUN\",\n",
      "    \"feats\": \"Gender=Fem|Number=Plur\",\n",
      "    \"head\": 3,\n",
      "    \"deprel\": \"nsubj\",\n",
      "    \"start_char\": 4,\n",
      "    \"end_char\": 14,\n",
      "    \"ner\": \"O\",\n",
      "    \"multi_ner\": [\n",
      "      \"O\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"text\": \"vont\",\n",
      "    \"lemma\": \"aller\",\n",
      "    \"upos\": \"VERB\",\n",
      "    \"feats\": \"Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "    \"head\": 0,\n",
      "    \"deprel\": \"root\",\n",
      "    \"start_char\": 15,\n",
      "    \"end_char\": 19,\n",
      "    \"ner\": \"O\",\n",
      "    \"multi_ner\": [\n",
      "      \"O\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"text\": \"de\",\n",
      "    \"lemma\": \"de\",\n",
      "    \"upos\": \"ADP\",\n",
      "    \"head\": 8,\n",
      "    \"deprel\": \"advmod\",\n",
      "    \"start_char\": 20,\n",
      "    \"end_char\": 22,\n",
      "    \"ner\": \"O\",\n",
      "    \"multi_ner\": [\n",
      "      \"O\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"text\": \"plus\",\n",
      "    \"lemma\": \"plus\",\n",
      "    \"upos\": \"ADV\",\n",
      "    \"head\": 4,\n",
      "    \"deprel\": \"fixed\",\n",
      "    \"start_char\": 23,\n",
      "    \"end_char\": 27,\n",
      "    \"ner\": \"O\",\n",
      "    \"multi_ner\": [\n",
      "      \"O\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 6,\n",
      "    \"text\": \"en\",\n",
      "    \"lemma\": \"en\",\n",
      "    \"upos\": \"ADP\",\n",
      "    \"head\": 4,\n",
      "    \"deprel\": \"fixed\",\n",
      "    \"start_char\": 28,\n",
      "    \"end_char\": 30,\n",
      "    \"ner\": \"O\",\n",
      "    \"multi_ner\": [\n",
      "      \"O\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 7,\n",
      "    \"text\": \"plus\",\n",
      "    \"lemma\": \"plus\",\n",
      "    \"upos\": \"ADV\",\n",
      "    \"head\": 4,\n",
      "    \"deprel\": \"fixed\",\n",
      "    \"start_char\": 31,\n",
      "    \"end_char\": 35,\n",
      "    \"ner\": \"O\",\n",
      "    \"multi_ner\": [\n",
      "      \"O\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 8,\n",
      "    \"text\": \"loin\",\n",
      "    \"lemma\": \"loin\",\n",
      "    \"upos\": \"ADV\",\n",
      "    \"head\": 3,\n",
      "    \"deprel\": \"advmod\",\n",
      "    \"start_char\": 36,\n",
      "    \"end_char\": 40,\n",
      "    \"ner\": \"O\",\n",
      "    \"multi_ner\": [\n",
      "      \"O\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 9,\n",
      "    \"text\": \".\",\n",
      "    \"lemma\": \".\",\n",
      "    \"upos\": \"PUNCT\",\n",
      "    \"head\": 3,\n",
      "    \"deprel\": \"punct\",\n",
      "    \"start_char\": 40,\n",
      "    \"end_char\": 41,\n",
      "    \"ner\": \"O\",\n",
      "    \"multi_ner\": [\n",
      "      \"O\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('LaD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e143b35b2151c3bc89ebda3b2f959c992381c54659ae3e56b021f94e69f457f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
