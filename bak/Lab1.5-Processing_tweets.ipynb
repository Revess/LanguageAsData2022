{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.3: Querying tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to query Twitter streams using the library *tweepy*. Take a look at its [documentation](https://github.com/tweepy/tweepy/tree/master/docs)\n",
    "\n",
    "Tweepy allows you to access Twitter using credentials and returns a so-called Cursor object. From the Cursor object, you can access the twitter data in e.g. JSON format. Documentation on the Twitter data objects can be found [here](https://developer.twitter.com/en/docs)\n",
    "\n",
    "\n",
    "Make sure you installed the package and obtained the Twitter credentials before your start using the API.\n",
    "\n",
    "https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up your twitter credentials to use the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all you need to have a standard Twitter account. It is easy to create a dummy account and you do not have to use your own name. It is okay, if you do not want to register for a Twitter account. In this case, you cannot test or modify the code. You need to make sure that you still understand how this works.\n",
    "\n",
    "1. Log in to your twitter account and go to developer.twitter.com\n",
    "2. Click on “Apply” in the top right and then on “Apply for a developer account”\n",
    "3. Choose “Academic”, then “Student” and “Get started” and fill in the required fields.\n",
    "4. Use the following text block for all text fields and mark questions 1 and 3 “yes“ and 2 and 4 \"no\":\n",
    "`Text Mining course at the VU university master program of the faculty of humanities. We analyse tweets for extracting data and information and obtaining statistics on language use. Analyses will be described in a password-protected blog. I am a student in this course\"`\n",
    "5. Read the Developer agreement and policy and agree (if you agree). Confirm the email and obtain the credentials.\n",
    "\n",
    "Set the constants API_KEY and API_Secret to your values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "# The API-Key and the API-secret were displayed to you after you registered\n",
    "API_KEY = ''\n",
    "API_SECRET = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Go to the developer portal, then Project and Apps, and create a Standalone App. Fill in a name for your app, it can be anything, e.g. ‘YOURNAME_Lab1”. Copy the access token and secret and store it in a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Access token and the Access secret were displayed when you clicked on \"generate\"\n",
    "ACCESS_TOKEN = ''\n",
    "ACCESS_SECRET = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Querying the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Tweepy to crawl tweets, but it is important to know that it has some limitations that affect reproducibility. The Twitter API is not exhaustive, it simply provides a sample and the documentation does not provide much detail on how this sample is determined. https://stackoverflow.com/questions/32445553/tweepy-not-finding-results-that-should-be-there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Twitter API returns the results as a JSON object. You learned how to use JSON objects in [Chapter 17](https://github.com/cltl/python-for-text-analysis/blob/master/Chapters/Chapter%2017%20-%20%20Data%20formats%20II%20(JSON).ipynb) of the Python course. The tweepy library makes it easier to access these JSON objects. \n",
    "\n",
    "The code below is used to set up the connection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Setup the authentication with your Twitter credentials:\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "# Create the api to connect to Twitter using your authentication \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a few variables to limit our search. Note that we can include hashtags and words in our keywords and combine them using Boolean operators such as OR and AND. Check the [Twitter API documentation](https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/overview/standard-operators) for more details on how to customize queries. \n",
    "\n",
    "**Play around with the parameters and understand how the queries are composed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the keywords in our target language\n",
    "language = \"en\"\n",
    "keywords = \"(#veganism OR #vegan) AND (children)\"\n",
    "\n",
    "# Optional: we can define a filter, for example, to ignore retweets\n",
    "filter = \"-filter:retweets\"\n",
    "\n",
    "query = keywords + filter\n",
    "\n",
    "# # Optional: Limit the number of tweets  \n",
    "count = 10\n",
    "\n",
    "# Request the tweets\n",
    "tweet_iterator = api.search_tweets(q=query,lang=language,count=count)\n",
    "\n",
    "# We save the tweets as a list, so that we can access them later. \n",
    "tweets = list(tweet_iterator)   \n",
    "\n",
    "for i, tweet in enumerate(tweets): \n",
    "    print(i)\n",
    "    print(\"User:\" + tweet.user.screen_name)\n",
    "    print(\"Tweet:\" + tweet.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Examining the attributes\n",
    "\n",
    "In the above code, we only check the username and the text of the tweet. The result that the API returns contains much more information that might be interesting for your analyses. Let's take a look at the attributes of the first tweet in our result list. \n",
    "\n",
    "**Discuss which of these properties would be interesting for your analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show all attributes of a tweet that you can access\n",
    "tweets[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all attributes of the user who wrote the tweet\n",
    "print(tweets[0].user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options for saving the results. \n",
    "1. We can select specific attributes and save them as a tsv-file. \n",
    "2. If we do not want to decide yet which attributes we need, we can simply dump the whole JSON result to a file and process it later. \n",
    "\n",
    "**Make sure that you understand the code below. Open the result files in an editor and compare the differences.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Collect the results\n",
    "tweets_as_json =[]\n",
    "tweets_as_text =[]\n",
    "\n",
    "for tweet in tweets: \n",
    "    \n",
    "    # Option 1: only keep selected attributes\n",
    "    text = tweet.text.replace(\"\\n\", \" \")\n",
    "    keep = str(tweet.created_at) + \"\\t\" + tweet.user.screen_name + \"\\t\" + text\n",
    "    tweets_as_text.append(keep)  \n",
    "    \n",
    "    # Option 2: keep everything and process later\n",
    "    tweets_as_json.append(tweet._json)\n",
    "    \n",
    "# Write them to a file\n",
    "csv_file = \"../results/twitter_search_results/results_veganism.csv\"\n",
    "json_file = \"../results/twitter_search_results/results_veganism.json\"\n",
    "\n",
    "with open(csv_file, 'w',encoding=\"utf-8\") as outfile:\n",
    "    csv_header = \"Created at\\tUser\\tText\\n\"\n",
    "    outfile.write(csv_header)\n",
    "    outfile.write(\"\\n\".join(tweets_as_text))\n",
    "\n",
    "with open(json_file, 'w') as outfile:\n",
    "    json.dump(tweets_as_json, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaDKernel",
   "language": "python",
   "name": "ladkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
