{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "from googletrans import Translator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9547072fc2af476abc102792c709745e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 15:18:47 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2022-11-30 15:18:48 INFO: File exists: /home/revess/stanza_resources/es/default.zip\n",
      "2022-11-30 15:18:53 INFO: Finished downloading models and saved to /home/revess/stanza_resources.\n",
      "2022-11-30 15:18:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6018baea012b493eb1bd617e50eb02b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 15:18:53 WARNING: Language es package default expects mwt, which has been added\n",
      "2022-11-30 15:18:53 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-11-30 15:18:53 INFO: Use device: cpu\n",
      "2022-11-30 15:18:53 INFO: Loading: tokenize\n",
      "2022-11-30 15:18:53 INFO: Loading: mwt\n",
      "2022-11-30 15:18:53 INFO: Loading: pos\n",
      "2022-11-30 15:18:53 INFO: Loading: lemma\n",
      "2022-11-30 15:18:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "\n",
    "for language in [\"nl\",\"es\":]\n",
    "    # Read in the data\n",
    "    article_file = \"./data/\"+language+\"Dataset.csv\"\n",
    "    content = pd.read_csv(article_file)\n",
    "\n",
    "    # Make sure to be very clear on how you determined the annotation terms.\n",
    "    # You can also use ngrams, e.g. \"vegan diet\" instead of \"diet\"\n",
    "    terms = [\"Trump\", \"trump\", \"Donald\"]\n",
    "\n",
    "    # Prepare the nlp pipeline\n",
    "    stanza.download(language)\n",
    "    nlp = stanza.Pipeline(language,  processors='tokenize,pos,lemma')\n",
    "\n",
    "    # Get all instances.\n",
    "    # Here, we do a simple string comparison.\n",
    "    # You might want to check for the lemma instead (depending on your research question)\n",
    "    annotation_instances = {}\n",
    "    for article in content[\"text\"]:\n",
    "        # Filter out empty articles\n",
    "        if len(article.strip())>0:\n",
    "            processed_article = nlp(article)\n",
    "            for sentence in processed_article.sentences:\n",
    "                for term in terms:\n",
    "                    if term in sentence.text:\n",
    "                        # Save all instances for each term\n",
    "                        # This is a condensed way of updating a dictionary. Make sure you understand what is happening\n",
    "                        annotation_instances[term] = annotation_instances.get(term, []) + [sentence.text.strip()]\n",
    "    num_annotators = 2\n",
    "    for term in terms:\n",
    "        df = {\"Term\":[],\"Instance\":[],\"Trans\":[],\"Annotation\":[]}\n",
    "        for instance in annotation_instances[term]:\n",
    "            instance = instance.replace(\"\\n\",\" \")\n",
    "            df[\"Instance\"].append(instance)\n",
    "            instance = translator.translate(instance, src=language, dest=\"en\").text\n",
    "            df[\"Trans\"].append(instance)\n",
    "            df[\"Term\"].append(term)\n",
    "            df[\"Annotation\"].append(\"x\")\n",
    "        for i in range(1, num_annotators + 1):\n",
    "            pd.DataFrame.from_dict(df).to_csv(\"./data/annotations/annotationsheet_\"+language+\"_\"+term+\"_a\"+str(i)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2 a1\n",
      "Percentage Agreement: 0.31\n",
      "Cohen's Kappa: -0.03\n",
      "[[36 41 49]\n",
      " [44 39 32]\n",
      " [42 41 39]]\n",
      "a2 a1\n",
      "Percentage Agreement: 0.57\n",
      "Cohen's Kappa: 0.35\n",
      "[[3 0 3]\n",
      " [0 5 3]\n",
      " [1 2 4]]\n",
      "a2 a1\n",
      "Percentage Agreement: 0.35\n",
      "Cohen's Kappa: 0.03\n",
      "[[50 41 49]\n",
      " [38 55 53]\n",
      " [52 40 45]]\n",
      "a1 a2\n",
      "Percentage Agreement: 0.34\n",
      "Cohen's Kappa: 0.01\n",
      "[[12 10 11]\n",
      " [ 6  8 12]\n",
      " [13 10 12]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import random\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "terms = [\"Trump\",\"Donald\"]\n",
    "categories = [1, -1, 0]\n",
    "\n",
    "# When we use the random function, we should set a fixed seed, if our results should be reproducible\n",
    "random.seed(733)\n",
    "\n",
    "for language in [\"nl\",\"es\"]:\n",
    "    for term in terms:\n",
    "        annotations = {}\n",
    "\n",
    "        # Read in the data\n",
    "        for sheet in glob.glob(\"./data/annotations/annotationsheet_\" + language + \"_\" + term +\"*.csv\"):\n",
    "            filename, extension = os.path.basename(sheet).split(\".\")\n",
    "            prefix, lang, term, annotator = filename.split(\"_\")\n",
    "\n",
    "            # Read in annotations\n",
    "            annotation_data = pd.read_csv(sheet, header=0)\n",
    "            annotations[annotator] = annotation_data[\"Annotation\"]\n",
    "\n",
    "            # The example sheets are not annotated. I am using random annotations here. Make sure to comment this part out.\n",
    "            annotations[annotator] = random.choices(categories, k=len(annotation_data))\n",
    "\n",
    "        annotators = annotations.keys()\n",
    "        # Calculate agreement between pairs of annotators. The order of the pair does not matter (a1,a2) is the same as a2,a1)\n",
    "        # With two annotators, you only have a single pair.\n",
    "        # If you have three annotators, you need to compare (a1,a2) (a1,a3) (a2,a3)\n",
    "\n",
    "        for annotator_a, annotator_b in combinations(annotators, 2):\n",
    "            agreement = [anno1 == anno2 for anno1, anno2 in  zip(annotations[annotator_a], annotations[annotator_b])]\n",
    "            percentage = sum(agreement)/len(agreement)\n",
    "            print(annotator_a, annotator_b)\n",
    "            print(\"Percentage Agreement: %.2f\" %percentage)\n",
    "            kappa = cohen_kappa_score(annotations[annotator_a], annotations[annotator_b], labels=categories)\n",
    "            print(\"Cohen's Kappa: %.2f\" %kappa)\n",
    "            confusions = confusion_matrix(annotations[annotator_a], annotations[annotator_b], labels=categories)\n",
    "            print(confusions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('LaD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e143b35b2151c3bc89ebda3b2f959c992381c54659ae3e56b021f94e69f457f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
